---
layout: post
title: "论文阅读小结"
date:  2025-07-05
tags: [LLM]
comments: true
author: 炼丹怪
pinned: true
---

> 本文持续更新，记录阅读的LLM相关论文及技术总结。

## 目录

- [1. Deep Research 系统综述](#1-deep-research-系统综述)

---

## 1. Deep Research 系统综述

**论文标题**: Deep Research: A Systematic Survey  
**论文链接**: [https://arxiv.org/abs/2512.02038](https://arxiv.org/abs/2512.02038)  

### 摘要

本文基于《Deep Research: A Systematic Survey》综述论文，对深度研究（Deep Research, DR）系统的技术架构、数学原理、优化方法及评估体系进行系统性总结。

### 1.1 核心定义与演进路线

**Deep Research (DR)** 定义为一种端到端的自主研究工作流，旨在赋予大语言模型（LLM）像人类研究员一样的能力：分解复杂问题、获取并筛选多源证据、管理长期记忆，并最终生成具有明确引用来源的长篇连贯报告。

**DR 与 RAG 的核心区别**：
* **传统 RAG**：静态检索、单次生成的“查询-回答”模式，依赖预索引语料库，缺乏长程规划。
* **Deep Research**：动态、多跳、具有长程规划能力的自主代理（Agent）。它不仅是检索，还包括假设生成、自我修正和全栈科学发现能力。

**演进路线**：
1.  **Phase I (Agentic Search)**：专注于准确的信息搜寻和检索（如 Search-R1）。
2.  **Phase II (Integrated Research)**：专注于长篇报告生成、证据合成与冲突处理。
3.  **Phase III (Full-stack AI Scientist)**：专注于自主提出假设、实验验证及科学发现（如 The AI Scientist）。

### 1.2 数学基础与优化算法

DR 系统的核心在于如何训练 Agent 进行自主推理和搜索。论文详细对比了 **PPO** 和 **GRPO** 两种强化学习算法在 DR 中的应用。

#### 1.2.1 强化学习优化目标
在 DR 中，RL 用于激励 Agent 进行多步推理和工具调用。

**Proximal Policy Optimization (PPO)**
PPO 是训练 Agent 的主流算法，通过限制策略更新幅度来保证稳定性。其目标函数为最大化裁剪后的代理优势：

$$L_{PPO}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]$$

* **符号定义**：
    * $\theta$: 当前策略参数。
    * $r_t(\theta) = \frac{\pi_{\theta}(o^t|s_t)}{\pi_{\theta_{old}}(o^t|s_t)}$: 概率比率。
    * $\hat{A}_t$: 估计的优势函数（Advantage），通常使用广义优势估计（GAE）计算。
    * $\epsilon$: 裁剪阈值，用于防止策略更新过大。

PPO 需要训练一个价值网络 $V_{\phi}(s_t)$ 来计算优势 $\hat{A}_t$，其损失函数为：
$$\mathcal{L}^{value}(\phi)=\frac{1}{2}\mathbb{E}_{t}[(V_{\phi}(s_{t})-\hat{R}_{t})^{2}]$$

**Group Relative Policy Optimization (GRPO)**
GRPO (由 DeepSeek 提出) 省去了价值网络，通过对同一查询生成的多个响应进行组内归一化来计算优势，降低了计算资源消耗。

对于同一查询 $q$，采样一组响应 $G = \{o_1, o_2, ..., o_m\}$，每个响应获得奖励 $\mathcal{R}_j$。GRPO 的组相对优势计算如下：

$$\hat{A}_{j}^{\mathcal{G}} = \frac{\mathcal{R}_j - \text{mean}(\{\mathcal{R}_i | i \in [m]\})}{\text{std}(\{\mathcal{R}_i | i \in [m]\}) + \epsilon}$$

* **技术优势**：GRPO 移除了对 Critic 模型的需求，专注于同一组输出的相对优劣，非常适合 DR 这种长链条推理场景。

#### 1.2.2 奖励函数设计
RL 的效果取决于奖励函数 $\mathcal{R}(\cdot)$ 的设计：
1.  **基于规则的奖励 (Rule-based Rewards)** $\mathcal{R}_{rule}(\cdot)$：适用于短答案，如 Exact Match (EM) 或 F1 分数。
2.  **LLM 作为裁判的奖励 (LLM-as-judge Rewards)** $\mathcal{R}_{LLMs}(\cdot)$：适用于长篇报告。计算公式为：
    $$\mathcal{R}_{LLMs}(o|q) = \mathbb{E}_{criteria \in \mathcal{C}} [\phi(o, q, criteria)]$$
    其中 $\mathcal{C}$ 包含准确性、完整性、引用质量等评估标准。

### 1.3 系统四大核心组件

#### 1.3.1 查询规划 (Query Planning)
将复杂问题分解为可执行的子查询序列。
* **并行规划 (Parallel Planning)**：一次性生成所有子查询。效率高，但忽略了子任务间的依赖关系（如 Least-to-Most Prompting）。
* **顺序规划 (Sequential Planning)**：迭代式分解，每一步依赖上一步的执行结果。适合多跳推理（如 Search-R1, R1-Searcher）。
* **基于树的规划 (Tree-based Planning)**：结合 MCTS（蒙特卡洛树搜索），在搜索空间中探索最优路径（如 RAG-Star 利用 UCT 算法选择最有潜力的推理节点）。

#### 1.3.2 信息获取 (Information Acquisition)
* **检索工具**：从传统的 BM25/Dense Retrieval 进化到使用商业搜索引擎（Google/Bing）API，以及具备视觉理解能力的多模态检索（解析图表、表格）。
* **检索时机 (Retrieval Timing)**：即“自适应检索”，解决何时检索的问题。
    * **概率策略**：当模型生成低置信度 Token 时触发检索。
    * **一致性策略**：当多次采样的一致性较低时触发。
    * **Agentic/RL 策略**：通过 RL 训练模型自主决定是否检索（如 Search-R1 将检索视为一种 Action）。
* **信息过滤**：
    * **重排序 (Selection)**：Point-wise, Pair-wise, List-wise (如 RankGPT)。
    * **压缩 (Compression)**：Context Compression，通过摘要或向量压缩减少 Token 占用。

#### 1.3.3 记忆管理 (Memory Management)
支持长周期研究的关键，分为四个操作阶段：
1.  **整合 (Consolidation)**：将原始交互转化为摘要、数据库元组或知识图谱。
2.  **索引 (Indexing)**：建立向量索引或图索引（Graph-based Indexing），支持多跳访问（如 HippoRAG）。
3.  **更新 (Updating)**：
    * *非参数更新*：直接修改外部数据库（Add/Update/Delete）。
    * *参数更新*：通过持续训练或编辑模型权重（Model Editing）。
4.  **遗忘 (Forgetting)**：主动删除过时或冲突信息（Active Forgetting），或基于遗忘曲线被动衰减。

#### 1.3.4 答案生成 (Answer Generation)
* **证据合成**：处理冲突信息，利用 Credibility-Aware Attention（基于来源可信度的注意力机制）或多智能体辩论（Multi-Agent Deliberation）。
* **长文连贯性**：论文提出了一个经验公式 $L_{model} \propto L_{SFT}$，即模型的最大连贯生成长度与 SFT 数据平均长度成正比，强调了长文本 SFT 数据的必要性。
* **结构化推理**：采用 Chain-of-Thought (CoT) 或 Plan-guided Writing（大纲引导写作）。
* **多模态生成**：生成包含图表、幻灯片甚至视频的报告。

### 1.4 训练与优化技术流水线

#### 1.4.1 工作流提示工程
利用多智能体编排（Orchestrator-Worker模式）。例如 Anthropic 的 Deep Research 系统，通过主控 Agent 进行任务分解、预算控制和子任务分发。

#### 1.4.2 监督微调 (SFT)
SFT 通常作为 RL 前的冷启动阶段。
* **强对弱蒸馏 (Strong-to-weak Distillation)**：利用 GPT-4 等强模型生成推理轨迹（Trajectory），蒸馏给小模型。
* **迭代自进化 (Iterative Self-Evolving)**：模型自我生成数据并进行微调，类似 Self-Rewarding 机制。

#### 1.4.3 端到端强化学习
这是当前最前沿的优化方向。
* **特定模块优化**：仅使用 RL 优化查询规划器（Planner），保持 QA 生成器冻结。
* **全流程优化**：将搜索、阅读、推理视为一个完整的马尔可夫决策过程（MDP），使用 PPO 或 GRPO 进行端到端训练。
    * **代表作**：Search-R1, DeepResearcher, R1-Searcher。
    * **挑战**：多轮交互中的“回声陷阱”（Echo Trap），即模型为了获得短期奖励而陷入保守、同质化的输出循环，导致探索能力下降。
    * **解决方案**：过滤无效轮次（Filtering void turns） 和设计更密集的奖励函数。

### 1.5 评估与挑战

#### 1.5.1 评估维度
1.  **信息搜寻 (Information Seeking)**：使用 GAIA, GPQA, Mind2Web 等基准，测试多跳推理和真实网页操作能力。
2.  **报告生成 (Report Generation)**：评估长文的连贯性、引用准确性（Citation Recall/Precision）。常用 ReportBench, DeepResearch Bench。
3.  **科研 AI (AI for Research)**：评估 Idea 的新颖性、实验代码的可执行性、论文评审质量。

#### 1.5.2 核心挑战
* **检索时机**：如何平衡过度检索（Over-retrieval）和检索不足（Under-retrieval）。
* **记忆进化**：目前的记忆多为被动存储，缺乏主动的个性化演进和认知结构的动态重组。
* **训练不稳定性**：多轮 RL 训练容易出现奖励稀疏和模型坍塌（Collapse）。
* **新颖性与幻觉的边界**：Deep Research 旨在发现新知，但这也增加了生成看似合理但无依据内容的风险（幻觉）。

---

## 添加新论文模板

```markdown
## N. 论文标题

**论文标题**: [英文标题]  
**论文链接**: [论文URL]  

### 摘要

[论文核心内容简述]

### N.1 主要章节1

[内容]

### N.2 主要章节2

[内容]

### N.3 核心贡献与启发

[总结]

---
```
