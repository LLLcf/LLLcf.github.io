---
layout: post
title: "论文阅读小结"
date:  2025-07-05
tags: [LLM]
comments: true
author: 炼丹怪
pinned: true
---

> 本文持续更新,记录阅读的LLM相关论文及技术总结。

---

<!-- more -->

## 目录

- [1. Deep Research](#1. DeepResearch)
- [2. Gated Attention](#2. Gated-Attention)
---

## 1. DeepResearch

**论文标题**: Deep Research: A Systematic Survey  

**论文链接**: [https://arxiv.org/abs/2512.02038](https://arxiv.org/abs/2512.02038)  

### 摘要

本文基于《Deep Research: A Systematic Survey》论文，对深度研究（Deep Research, DR）系统的技术架构、数学原理、优化方法及评估体系进行系统性总结。

### 1.1 核心定义与演进路线

**Deep Research (DR)** 定义为一种端到端的自主研究工作流，旨在赋予大语言模型（LLM）像人类研究员一样的能力：分解复杂问题、获取并筛选多源证据、管理长期记忆，并最终生成具有明确引用来源的长篇连贯报告。

**DR 与 RAG 的核心区别**：
* **传统 RAG**：静态检索、单次生成的“查询-回答”模式，依赖预索引语料库，缺乏长程规划。
* **Deep Research**：动态、多跳、具有长程规划能力的自主代理（Agent）。它不仅是检索，还包括假设生成、自我修正和全栈科学发现能力。

**演进路线**：
1.  **Phase I (Agentic Search)**：专注于准确的信息搜寻和检索（如 Search-R1）。
2.  **Phase II (Integrated Research)**：专注于长篇报告生成、证据合成与冲突处理。
3.  **Phase III (Full-stack AI Scientist)**：专注于自主提出假设、实验验证及科学发现（如 The AI Scientist）。

### 1.2 数学基础与优化算法

DR 系统的核心在于如何训练 Agent 进行自主推理和搜索。论文详细对比了 **PPO** 和 **GRPO** 两种强化学习算法在 DR 中的应用。

#### 1.2.1 强化学习优化目标
在 DR 中，RL 用于激励 Agent 进行多步推理和工具调用。

**Proximal Policy Optimization (PPO)**
PPO 是训练 Agent 的主流算法，通过限制策略更新幅度来保证稳定性。其目标函数为最大化裁剪后的代理优势：

$$L_{PPO}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]$$

**符号定义**：

1.当前策略参数:$\theta$

2.概率比率:
$$
r_t(\theta) = \frac{\pi_{\theta}(o^t|s_t)}{\pi_{\theta_{old}}(o^t|s_t)}
$$

3.$\hat{A}_t$: 估计的优势函数（Advantage），通常使用广义优势估计（GAE）计算。

4.$\epsilon$: 裁剪阈值，用于防止策略更新过大。

PPO 需要训练一个价值网络 $V_{\phi}(s_t)$ 来计算优势 $\hat{A}_t$，其损失函数为：

$$
\mathcal{L}^{value}(\phi)=\frac{1}{2}\mathbb{E}_{t}[(V_{\phi}(s_{t})-\hat{R}_{t})^{2}]
$$

**Group Relative Policy Optimization (GRPO)**
GRPO (由 DeepSeek 提出) 省去了价值网络，通过对同一查询生成的多个响应进行组内归一化来计算优势，降低了计算资源消耗。

对于同一查询 $q$，采样一组响应 $G = \{o_1, o_2, ..., o_m\}$，每个响应获得奖励 $\mathcal{R}_j$。GRPO 的组相对优势计算如下：

$$\hat{A}_{j}^{\mathcal{G}} = \frac{\mathcal{R}_j - \text{mean}(\{\mathcal{R}_i | i \in [m]\})}{\text{std}(\{\mathcal{R}_i | i \in [m]\}) + \epsilon}$$

* **技术优势**：GRPO 移除了对 Critic 模型的需求，专注于同一组输出的相对优劣，非常适合 DR 这种长链条推理场景。

#### 1.2.2 奖励函数设计
RL 的效果取决于奖励函数 $\mathcal{R}(\cdot)$ 的设计：

1.**基于规则的奖励 (Rule-based Rewards)** $\mathcal{R}_{rule}(\cdot)$：适用于短答案，如 Exact Match (EM) 或 F1 分数。

2.**LLM 作为裁判的奖励 (LLM-as-judge Rewards)** $\mathcal{R}_{LLMs}(\cdot)$：适用于长篇报告。计算公式为：

$$\mathcal{R}_{LLMs}(o|q) = \mathbb{E}_{criteria \in \mathcal{C}} [\phi(o, q, criteria)]
$$

其中 $\mathcal{C}$ 包含准确性、完整性、引用质量等评估标准。

### 1.3 系统四大核心组件

#### 1.3.1 查询规划 (Query Planning)
将复杂问题分解为可执行的子查询序列。
* **并行规划 (Parallel Planning)**：一次性生成所有子查询。效率高，但忽略了子任务间的依赖关系（如 Least-to-Most Prompting）。
* **顺序规划 (Sequential Planning)**：迭代式分解，每一步依赖上一步的执行结果。适合多跳推理（如 Search-R1, R1-Searcher）。
* **基于树的规划 (Tree-based Planning)**：结合 MCTS（蒙特卡洛树搜索），在搜索空间中探索最优路径（如 RAG-Star 利用 UCT 算法选择最有潜力的推理节点）。

#### 1.3.2 信息获取 (Information Acquisition)
* **检索工具**：从传统的 BM25/Dense Retrieval 进化到使用商业搜索引擎（Google/Bing）API，以及具备视觉理解能力的多模态检索（解析图表、表格）。
* **检索时机 (Retrieval Timing)**：即“自适应检索”，解决何时检索的问题。
    * **概率策略**：当模型生成低置信度 Token 时触发检索。
    * **一致性策略**：当多次采样的一致性较低时触发。
    * **Agentic/RL 策略**：通过 RL 训练模型自主决定是否检索（如 Search-R1 将检索视为一种 Action）。
* **信息过滤**：
    * **重排序 (Selection)**：Point-wise, Pair-wise, List-wise (如 RankGPT)。
    * **压缩 (Compression)**：Context Compression，通过摘要或向量压缩减少 Token 占用。

#### 1.3.3 记忆管理 (Memory Management)
支持长周期研究的关键，分为四个操作阶段：
1.  **整合 (Consolidation)**：将原始交互转化为摘要、数据库元组或知识图谱。
2.  **索引 (Indexing)**：建立向量索引或图索引（Graph-based Indexing），支持多跳访问（如 HippoRAG）。
3.  **更新 (Updating)**：
    * *非参数更新*：直接修改外部数据库（Add/Update/Delete）。
    * *参数更新*：通过持续训练或编辑模型权重（Model Editing）。
4.  **遗忘 (Forgetting)**：主动删除过时或冲突信息（Active Forgetting），或基于遗忘曲线被动衰减。

#### 1.3.4 答案生成 (Answer Generation)
* **证据合成**：处理冲突信息，利用 Credibility-Aware Attention（基于来源可信度的注意力机制）或多智能体辩论（Multi-Agent Deliberation）。
* **长文连贯性**：论文提出了一个经验公式 $L_{model} \propto L_{SFT}$，即模型的最大连贯生成长度与 SFT 数据平均长度成正比，强调了长文本 SFT 数据的必要性。
* **结构化推理**：采用 Chain-of-Thought (CoT) 或 Plan-guided Writing（大纲引导写作）。
* **多模态生成**：生成包含图表、幻灯片甚至视频的报告。

### 1.4 训练与优化技术流水线

#### 1.4.1 工作流提示工程
利用多智能体编排（Orchestrator-Worker模式）。例如 Anthropic 的 Deep Research 系统，通过主控 Agent 进行任务分解、预算控制和子任务分发。

#### 1.4.2 监督微调 (SFT)
SFT 通常作为 RL 前的冷启动阶段。
* **强对弱蒸馏 (Strong-to-weak Distillation)**：利用 GPT-4 等强模型生成推理轨迹（Trajectory），蒸馏给小模型。
* **迭代自进化 (Iterative Self-Evolving)**：模型自我生成数据并进行微调，类似 Self-Rewarding 机制。

#### 1.4.3 端到端强化学习
这是当前最前沿的优化方向。
* **特定模块优化**：仅使用 RL 优化查询规划器（Planner），保持 QA 生成器冻结。
* **全流程优化**：将搜索、阅读、推理视为一个完整的马尔可夫决策过程（MDP），使用 PPO 或 GRPO 进行端到端训练。
    * **代表作**：Search-R1, DeepResearcher, R1-Searcher。
    * **挑战**：多轮交互中的“回声陷阱”（Echo Trap），即模型为了获得短期奖励而陷入保守、同质化的输出循环，导致探索能力下降。
    * **解决方案**：过滤无效轮次（Filtering void turns） 和设计更密集的奖励函数。

### 1.5 评估与挑战

#### 1.5.1 评估维度
1.  **信息搜寻 (Information Seeking)**：使用 GAIA, GPQA, Mind2Web 等基准，测试多跳推理和真实网页操作能力。
2.  **报告生成 (Report Generation)**：评估长文的连贯性、引用准确性（Citation Recall/Precision）。常用 ReportBench, DeepResearch Bench。
3.  **科研 AI (AI for Research)**：评估 Idea 的新颖性、实验代码的可执行性、论文评审质量。

#### 1.5.2 核心挑战
* **检索时机**：如何平衡过度检索（Over-retrieval）和检索不足（Under-retrieval）。
* **记忆进化**：目前的记忆多为被动存储，缺乏主动的个性化演进和认知结构的动态重组。
* **训练不稳定性**：多轮 RL 训练容易出现奖励稀疏和模型坍塌（Collapse）。
* **新颖性与幻觉的边界**：Deep Research 旨在发现新知，但这也增加了生成看似合理但无依据内容的风险（幻觉）。

---

## 2. Gated Attention

**论文标题**: Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free

**论文链接**: [(https://openreview.net/pdf?id=1b7whO4SfY)](https://openreview.net/pdf?id=1b7whO4SfY)

### 摘要

针对现有 Softmax Attention 机制在大模型训练中面临的训练不稳定、Attention Sink（注意力陷阱）以及线性表达能力受限等问题，Qwen（通义千问）团队提出了一种 Gated Attention（门控注意力） 机制 。该研究发现，在 Scaled Dot-Product Attention (SDPA) 输出后应用一个简单的头部特定（head-specific）Sigmoid 门控，能够显著提升模型性能、训练稳定性和上下文外推能力 。该方法不仅消除了 Attention Sink 现象，还支持更大的学习率训练 ，且已被应用于 Qwen3-Next 模型的研发中 。

### 2.1 背景与动机

#### 2.1.1 现有 Transformer 架构的痛点

在训练大规模语言模型时,现有的 Softmax Attention 机制面临三个主要挑战:

* **训练稳定性问题**:随着模型深度增加或学习率提高,训练损失(Loss)容易出现剧烈震荡(Loss Spikes),导致训练发散或收敛困难 。
* **Attention Sink(注意力陷阱)**:模型倾向于将大量注意力权重分配给第一个 Token(即使该 Token 无实际语义),导致长文本推理和外推能力受限。
* **线性表达瓶颈**:在标准 Attention 结构中,Value 投影 ($W_V$) 和 输出投影 ($W_O$) 是两个连续的线性层,这种低秩(Low-rank)线性映射限制了模型的特征表达能力。

#### 2.1.2 核心贡献

Qwen 团队提出了 **Gated Attention(门控注意力)** 机制。通过在 Attention 输出端引入门控,实现了以下改进:

* **性能提升**:在 15B MoE 模型和 1.7B Dense 模型上,困惑度(PPL)显著降低,MMLU 等下游任务指标提升 。
* **消除 Attention Sink**:通过输入依赖的稀疏性,消除了首个 Token 的注意力聚集现象 。
* **增强训练稳定性**:有效平滑了 Loss 曲线,允许使用更大的学习率进行训练,从而加速收敛。
* **参数高效**:该方法作为一种底层的架构改进,引入的参数量微乎其微(小于 2%),但能显著改善模型的训练动态和长文能力。

### 2.2 技术原理

#### 2.2.1 最佳门控位置

论文对比了在 Attention 层不同位置(Query/Key/Value 投影后、SDPA 输出后、最终输出后)引入门控的效果。实验表明,**在 SDPA(Scaled Dot-Product Attention)输出之后引入门控 ($G_1$) 效果最佳** 。

**标准路径**:$Input \to Q,K,V \to \text{SDPA}(Q,K,V) \to W_O \to Output$

**Gated Attention 路径**:$Input \to Q,K,V \to \underbrace{\text{SDPA}(Q,K,V)}_{\text{Attention Output}} \xrightarrow{\text{Gating}} \text{Modulated Output} \to W_O \to Output$

#### 2.2.2 关键设计要素

**1. SDPA Output Gating 机制**
在 SDPA 计算完成后，通过一个门控模块对输出进行动态调制。

**公式定义**：
$$O = \text{SDPA}(Q, K, V) \odot \sigma(X W_{\theta})$$

其中 $\odot$ 表示逐元素乘法（Element-wise），$\sigma$ 为 Sigmoid 激活函数，$W_{\theta}$ 为门控参数。

**作用机理**：
门控模块作为一个**输入依赖（Query-dependent）的动态滤波器**。它根据当前的输入 Query，计算出每个特征维度的重要性分数（0~1）：接近 1：保留该特征信息。接近 0：抑制噪音或无关信息（如 Attention Sink 带来的冗余激活）。

**2. 激活函数与粒度选择**
论文通过消融实验确定了最优配置：
1.**激活函数**：**Sigmoid 优于 SiLU**。Sigmoid 将输出限制在 [0, 1] 区间，能够产生更强的稀疏性（大量门控值接近 0），这对消除噪音至关重要。

2.**粒度（Granularity）**：**Element-wise（逐元素）优于 Head-wise（逐头）**。对每个 Head 的每个特征维度独立门控，能提供更精细的控制。

3.**参数共享**：**Head-Specific（每头独立）优于 Head-Shared（多头共享）**。不同 Head 捕捉的模式不同，需要独立的门控参数。

**3. 稀疏性与 Attention Sink 的消除**

* **输入依赖稀疏性**：实验观察到，训练后的门控分数呈现显著的稀疏分布（均值较低且集中在 0 附近）。这种稀疏性是依赖于输入 Query 的，即模型学会了根据当前上下文需求，选择性地关闭大部分无关信息的通道。
* **消除 Attention Sink**：在引入 Gated Attention 后，首个 Token 的注意力占比从 Baseline 的 **46.7% 大幅下降至 4.8%**。这证明了门控机制能有效过滤掉仅用于维持数值稳定的无效注意力。

#### 2.2.3 与现有方法的关联

* **非线性增强**:标准 Attention 中 $W_V$ 和 $W_O$ 构成了连续的线性变换。引入非线性的门控机制增加了低秩映射的表达能力。
* **机制溯源**:该设计理念可追溯至 LSTM 中的"遗忘门"机制,即通过门控来调节信息流。

### 2.3 实验结果

#### 2.3.1 核心代码实现

```python
class GatedSelfAttention(nn.Module):
    def forward(self, x, freqs_cis):
        # 1. 计算 Query, Key, Value
        q, k, v = self.qkv_proj(x).split(...)
        
        # 2. 执行标准 SDPA
        # attn_output: [batch, seq_len, num_heads, head_dim]
        attn_output = self.sdpa(q, k, v) 
        
        # 3. 计算 Gate (论文核心 G1: SDPA Output Gating)
        # gate_map: [batch, seq_len, num_heads, head_dim]
        # 注意：使用 Sigmoid 激活函数，且参数为 Head-Specific
        gate_map = torch.sigmoid(self.gate_linear(x)) 
        
        # 4. 门控融合 (Element-wise multiplication)
        gated_output = attn_output * gate_map
        
        # 5. 最终输出投影
        return self.o_proj(gated_output)
```

#### 2.3.2 性能对比
在 15B 参数的 MoE 模型（15A2B）上进行的实验显示：
* **PPL 提升**：Gated Attention 模型的 PPL 从 Baseline 的 6.026 降低至 5.761。
* **下游任务**：在 MMLU（通用知识）、GSM8k（数学）、HumanEval（代码）等基准测试中均取得了性能提升。

#### 2.3.3 训练稳定性

实验对比了 Baseline 和 Gated Attention 在不同学习率下的训练曲线：
* **Baseline**：在高学习率下容易出现 Loss Spikes，导致训练不稳定。
* **Gated Attention**：显著平滑了 Loss 曲线，几乎消除了 Loss Spikes。这使得模型能够支持更大的 Batch Size 和更高的学习率，从而提升训练效率。

#### 2.3.4 长文本外推能力

在通过 YaRN 技术将上下文窗口从 4k 扩展至 128k 的实验中:

* **Baseline**:随着长度增加,性能出现明显下降,部分原因归结于 Attention Sink 机制在超长上下文下的失效。
* **Gated Attention**:得益于去除了 Attention Sink,模型在长上下文(64k, 128k)下的表现显著优于 Baseline,展现出更强的外推能力。

### 2.4 实施建议

基于论文结论,对于大模型架构优化的建议如下:

1. **推荐配置**:在 Attention 层中,于 SDPA 输出后、输出投影前,插入 **Element-wise Sigmoid Gating**。
2. **超参调整**:由于该结构增强了稳定性,建议在训练时尝试 **适度增大学习率** 以获得更好的收敛效果。
3. **监控指标**:在实验中,可通过观察 Loss 曲线的平滑度以及 Attention Map 中首个 Token 的权重变化来验证改进效果。

---
