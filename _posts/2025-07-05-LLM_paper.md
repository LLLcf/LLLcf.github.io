---
layout: post
title: "论文阅读小结"
date:  2025-07-05
tags: [LLM]
comments: true
author: 炼丹怪
pinned: true
---

> 本文持续更新,记录阅读的LLM相关论文及技术总结。

---

<!-- more -->

## 目录

[Deep Research: A Systematic Survey](#1-deep-research-a-systematic-survey)

[Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](#2-gated-attention-for-large-language-models-non-linearity-sparsity-and-attention-sink-free)

[Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](#3-does-reinforcement-learning-really-incentivize-reasoning-capacity-in-llms-beyond-the-base-model)

[Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values](#4-every-question-has-its-own-value-reinforcement-learning-with-explicit-human-values)

[DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models](#5-deepseek-v32-pushing-the-frontier-of-open-large-language-models)

[Training LLMs for Honesty via Confessions](#6-training-llms-for-honesty-via-confessions)

[The Art of Scaling Reinforcement Learning Compute for LLMs](#7-the-art-of-scaling-reinforcement-learning-compute-for-llms)

[Twilight: Adaptive Attention Sparsity with Hierarchical Top-p Pruning](#8-twilight-adaptive-attention-sparsity-with-hierarchical-top-p-pruning)


---


## 1. Deep Research: A Systematic Survey

**论文链接**: [https://arxiv.org/abs/2512.02038](https://arxiv.org/abs/2512.02038)  

### 摘要

本文基于《Deep Research: A Systematic Survey》论文，对深度研究（Deep Research, DR）系统的技术架构、数学原理、优化方法及评估体系进行系统性总结。

### 1.1 核心定义与演进路线

**Deep Research (DR)** 定义为一种端到端的自主研究工作流，旨在赋予大语言模型（LLM）像人类研究员一样的能力：分解复杂问题、获取并筛选多源证据、管理长期记忆，并最终生成具有明确引用来源的长篇连贯报告。

**DR 与 RAG 的核心区别**：
* **传统 RAG**：静态检索、单次生成的“查询-回答”模式，依赖预索引语料库，缺乏长程规划。
* **Deep Research**：动态、多跳、具有长程规划能力的自主代理（Agent）。它不仅是检索，还包括假设生成、自我修正和全栈科学发现能力。

**演进路线**：
1.  **Phase I (Agentic Search)**：专注于准确的信息搜寻和检索（如 Search-R1）。
2.  **Phase II (Integrated Research)**：专注于长篇报告生成、证据合成与冲突处理。
3.  **Phase III (Full-stack AI Scientist)**：专注于自主提出假设、实验验证及科学发现（如 The AI Scientist）。

### 1.2 数学基础与优化算法

DR 系统的核心在于如何训练 Agent 进行自主推理和搜索。论文详细对比了 **PPO** 和 **GRPO** 两种强化学习算法在 DR 中的应用。

#### 1.2.1 强化学习优化目标
在 DR 中，RL 用于激励 Agent 进行多步推理和工具调用。

**Proximal Policy Optimization (PPO)**
PPO 是训练 Agent 的主流算法，通过限制策略更新幅度来保证稳定性。其目标函数为最大化裁剪后的代理优势：

$$L_{PPO}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]$$

**符号定义**：

1.当前策略参数:$\theta$

2.概率比率:
$$
r_t(\theta) = \frac{\pi_{\theta}(o^t|s_t)}{\pi_{\theta_{old}}(o^t|s_t)}
$$

3.$\hat{A}_t$: 估计的优势函数（Advantage），通常使用广义优势估计（GAE）计算。

4.$\epsilon$: 裁剪阈值，用于防止策略更新过大。

PPO 需要训练一个价值网络 $V_{\phi}(s_t)$ 来计算优势 $\hat{A}_t$，其损失函数为：

$$
\mathcal{L}^{value}(\phi)=\frac{1}{2}\mathbb{E}_{t}[(V_{\phi}(s_{t})-\hat{R}_{t})^{2}]
$$

**Group Relative Policy Optimization (GRPO)**
GRPO (由 DeepSeek 提出) 省去了价值网络，通过对同一查询生成的多个响应进行组内归一化来计算优势，降低了计算资源消耗。

对于同一查询 $q$，采样一组响应 $G = \{o_1, o_2, ..., o_m\}$，每个响应获得奖励 $\mathcal{R}_j$。GRPO 的组相对优势计算如下：

$$\hat{A}_{j}^{\mathcal{G}} = \frac{\mathcal{R}_j - \text{mean}(\{\mathcal{R}_i | i \in [m]\})}{\text{std}(\{\mathcal{R}_i | i \in [m]\}) + \epsilon}$$

* **技术优势**：GRPO 移除了对 Critic 模型的需求，专注于同一组输出的相对优劣，非常适合 DR 这种长链条推理场景。

#### 1.2.2 奖励函数设计
RL 的效果取决于奖励函数 $\mathcal{R}(\cdot)$ 的设计：

1.**基于规则的奖励 (Rule-based Rewards)** $\mathcal{R}_{rule}(\cdot)$：适用于短答案，如 Exact Match (EM) 或 F1 分数。

2.**LLM 作为裁判的奖励 (LLM-as-judge Rewards)** $\mathcal{R}_{LLMs}(\cdot)$：适用于长篇报告。计算公式为：

$$\mathcal{R}_{LLMs}(o|q) = \mathbb{E}_{criteria \in \mathcal{C}} [\phi(o, q, criteria)]
$$

其中 $\mathcal{C}$ 包含准确性、完整性、引用质量等评估标准。

### 1.3 系统四大核心组件

#### 1.3.1 查询规划 (Query Planning)
将复杂问题分解为可执行的子查询序列。
* **并行规划 (Parallel Planning)**：一次性生成所有子查询。效率高，但忽略了子任务间的依赖关系（如 Least-to-Most Prompting）。
* **顺序规划 (Sequential Planning)**：迭代式分解，每一步依赖上一步的执行结果。适合多跳推理（如 Search-R1, R1-Searcher）。
* **基于树的规划 (Tree-based Planning)**：结合 MCTS（蒙特卡洛树搜索），在搜索空间中探索最优路径（如 RAG-Star 利用 UCT 算法选择最有潜力的推理节点）。

#### 1.3.2 信息获取 (Information Acquisition)
* **检索工具**：从传统的 BM25/Dense Retrieval 进化到使用商业搜索引擎（Google/Bing）API，以及具备视觉理解能力的多模态检索（解析图表、表格）。
* **检索时机 (Retrieval Timing)**：即“自适应检索”，解决何时检索的问题。
    * **概率策略**：当模型生成低置信度 Token 时触发检索。
    * **一致性策略**：当多次采样的一致性较低时触发。
    * **Agentic/RL 策略**：通过 RL 训练模型自主决定是否检索（如 Search-R1 将检索视为一种 Action）。
* **信息过滤**：
    * **重排序 (Selection)**：Point-wise, Pair-wise, List-wise (如 RankGPT)。
    * **压缩 (Compression)**：Context Compression，通过摘要或向量压缩减少 Token 占用。

#### 1.3.3 记忆管理 (Memory Management)
支持长周期研究的关键，分为四个操作阶段：
1.  **整合 (Consolidation)**：将原始交互转化为摘要、数据库元组或知识图谱。
2.  **索引 (Indexing)**：建立向量索引或图索引（Graph-based Indexing），支持多跳访问（如 HippoRAG）。
3.  **更新 (Updating)**：
    * *非参数更新*：直接修改外部数据库（Add/Update/Delete）。
    * *参数更新*：通过持续训练或编辑模型权重（Model Editing）。
4.  **遗忘 (Forgetting)**：主动删除过时或冲突信息（Active Forgetting），或基于遗忘曲线被动衰减。

#### 1.3.4 答案生成 (Answer Generation)
* **证据合成**：处理冲突信息，利用 Credibility-Aware Attention（基于来源可信度的注意力机制）或多智能体辩论（Multi-Agent Deliberation）。
* **长文连贯性**：论文提出了一个经验公式 $L_{model} \propto L_{SFT}$，即模型的最大连贯生成长度与 SFT 数据平均长度成正比，强调了长文本 SFT 数据的必要性。
* **结构化推理**：采用 Chain-of-Thought (CoT) 或 Plan-guided Writing（大纲引导写作）。
* **多模态生成**：生成包含图表、幻灯片甚至视频的报告。

### 1.4 训练与优化技术流水线

#### 1.4.1 工作流提示工程
利用多智能体编排（Orchestrator-Worker模式）。例如 Anthropic 的 Deep Research 系统，通过主控 Agent 进行任务分解、预算控制和子任务分发。

#### 1.4.2 监督微调 (SFT)
SFT 通常作为 RL 前的冷启动阶段。
* **强对弱蒸馏 (Strong-to-weak Distillation)**：利用 GPT-4 等强模型生成推理轨迹（Trajectory），蒸馏给小模型。
* **迭代自进化 (Iterative Self-Evolving)**：模型自我生成数据并进行微调，类似 Self-Rewarding 机制。

#### 1.4.3 端到端强化学习
这是当前最前沿的优化方向。
* **特定模块优化**：仅使用 RL 优化查询规划器（Planner），保持 QA 生成器冻结。
* **全流程优化**：将搜索、阅读、推理视为一个完整的马尔可夫决策过程（MDP），使用 PPO 或 GRPO 进行端到端训练。
    * **代表作**：Search-R1, DeepResearcher, R1-Searcher。
    * **挑战**：多轮交互中的“回声陷阱”（Echo Trap），即模型为了获得短期奖励而陷入保守、同质化的输出循环，导致探索能力下降。
    * **解决方案**：过滤无效轮次（Filtering void turns） 和设计更密集的奖励函数。

### 1.5 评估与挑战

#### 1.5.1 评估维度
1.  **信息搜寻 (Information Seeking)**：使用 GAIA, GPQA, Mind2Web 等基准，测试多跳推理和真实网页操作能力。
2.  **报告生成 (Report Generation)**：评估长文的连贯性、引用准确性（Citation Recall/Precision）。常用 ReportBench, DeepResearch Bench。
3.  **科研 AI (AI for Research)**：评估 Idea 的新颖性、实验代码的可执行性、论文评审质量。

#### 1.5.2 核心挑战
* **检索时机**：如何平衡过度检索（Over-retrieval）和检索不足（Under-retrieval）。
* **记忆进化**：目前的记忆多为被动存储，缺乏主动的个性化演进和认知结构的动态重组。
* **训练不稳定性**：多轮 RL 训练容易出现奖励稀疏和模型坍塌（Collapse）。
* **新颖性与幻觉的边界**：Deep Research 旨在发现新知，但这也增加了生成看似合理但无依据内容的风险（幻觉）。

---

## 2. Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free

**论文链接**: [https://openreview.net/pdf?id=1b7whO4SfY](https://openreview.net/pdf?id=1b7whO4SfY)

### 摘要

针对现有 Softmax Attention 机制在大模型训练中面临的训练不稳定、Attention Sink（注意力陷阱）以及线性表达能力受限等问题，Qwen（通义千问）团队提出了一种 Gated Attention（门控注意力） 机制 。该研究发现，在 Scaled Dot-Product Attention (SDPA) 输出后应用一个简单的头部特定（head-specific）Sigmoid 门控，能够显著提升模型性能、训练稳定性和上下文外推能力 。该方法不仅消除了 Attention Sink 现象，还支持更大的学习率训练 ，且已被应用于 Qwen3-Next 模型的研发中 。

### 2.1 背景与动机

#### 2.1.1 现有 Transformer 架构的痛点

在训练大规模语言模型时,现有的 Softmax Attention 机制面临三个主要挑战:

* **训练稳定性问题**:随着模型深度增加或学习率提高,训练损失(Loss)容易出现剧烈震荡(Loss Spikes),导致训练发散或收敛困难 。
* **Attention Sink(注意力陷阱)**:模型倾向于将大量注意力权重分配给第一个 Token(即使该 Token 无实际语义),导致长文本推理和外推能力受限。
* **线性表达瓶颈**:在标准 Attention 结构中,Value 投影 ($W_V$) 和 输出投影 ($W_O$) 是两个连续的线性层,这种低秩(Low-rank)线性映射限制了模型的特征表达能力。

#### 2.1.2 核心贡献

Qwen 团队提出了 **Gated Attention(门控注意力)** 机制。通过在 Attention 输出端引入门控,实现了以下改进:

* **性能提升**:在 15B MoE 模型和 1.7B Dense 模型上,困惑度(PPL)显著降低,MMLU 等下游任务指标提升 。
* **消除 Attention Sink**:通过输入依赖的稀疏性,消除了首个 Token 的注意力聚集现象 。
* **增强训练稳定性**:有效平滑了 Loss 曲线,允许使用更大的学习率进行训练,从而加速收敛。
* **参数高效**:该方法作为一种底层的架构改进,引入的参数量微乎其微(小于 2%),但能显著改善模型的训练动态和长文能力。

### 2.2 技术原理

#### 2.2.1 最佳门控位置

论文对比了在 Attention 层不同位置(Query/Key/Value 投影后、SDPA 输出后、最终输出后)引入门控的效果。实验表明,**在 SDPA(Scaled Dot-Product Attention)输出之后引入门控 ($G_1$) 效果最佳** 。

**标准路径**:$Input \to Q,K,V \to \text{SDPA}(Q,K,V) \to W_O \to Output$

**Gated Attention 路径**:$Input \to Q,K,V \to \underbrace{\text{SDPA}(Q,K,V)}_{\text{Attention Output}} \xrightarrow{\text{Gating}} \text{Modulated Output} \to W_O \to Output$

#### 2.2.2 关键设计要素

**1. SDPA Output Gating 机制**
在 SDPA 计算完成后，通过一个门控模块对输出进行动态调制。

**公式定义**：
$$O = \text{SDPA}(Q, K, V) \odot \sigma(X W_{\theta})$$

其中 $\odot$ 表示逐元素乘法（Element-wise），$\sigma$ 为 Sigmoid 激活函数，$W_{\theta}$ 为门控参数。

**作用机理**：
门控模块作为一个**输入依赖（Query-dependent）的动态滤波器**。它根据当前的输入 Query，计算出每个特征维度的重要性分数（0~1）：接近 1：保留该特征信息。接近 0：抑制噪音或无关信息（如 Attention Sink 带来的冗余激活）。

**2. 激活函数与粒度选择**
论文通过消融实验确定了最优配置：
1.**激活函数**：**Sigmoid 优于 SiLU**。Sigmoid 将输出限制在 [0, 1] 区间，能够产生更强的稀疏性（大量门控值接近 0），这对消除噪音至关重要。

2.**粒度（Granularity）**：**Element-wise（逐元素）优于 Head-wise（逐头）**。对每个 Head 的每个特征维度独立门控，能提供更精细的控制。

3.**参数共享**：**Head-Specific（每头独立）优于 Head-Shared（多头共享）**。不同 Head 捕捉的模式不同，需要独立的门控参数。

**3. 稀疏性与 Attention Sink 的消除**

* **输入依赖稀疏性**：实验观察到，训练后的门控分数呈现显著的稀疏分布（均值较低且集中在 0 附近）。这种稀疏性是依赖于输入 Query 的，即模型学会了根据当前上下文需求，选择性地关闭大部分无关信息的通道。
* **消除 Attention Sink**：在引入 Gated Attention 后，首个 Token 的注意力占比从 Baseline 的 **46.7% 大幅下降至 4.8%**。这证明了门控机制能有效过滤掉仅用于维持数值稳定的无效注意力。

#### 2.2.3 与现有方法的关联

* **非线性增强**:标准 Attention 中 $W_V$ 和 $W_O$ 构成了连续的线性变换。引入非线性的门控机制增加了低秩映射的表达能力。
* **机制溯源**:该设计理念可追溯至 LSTM 中的"遗忘门"机制,即通过门控来调节信息流。

### 2.3 实验结果

#### 2.3.1 伪代码实现

```python
class GatedSelfAttention(nn.Module):
    def forward(self, x, freqs_cis):
        # 1. 计算 Query, Key, Value
        q, k, v = self.qkv_proj(x).split(...)
        
        # 2. 执行标准 SDPA
        # attn_output: [batch, seq_len, num_heads, head_dim]
        attn_output = self.sdpa(q, k, v) 
        
        # 3. 计算 Gate (论文核心 G1: SDPA Output Gating)
        # gate_map: [batch, seq_len, num_heads, head_dim]
        # 注意：使用 Sigmoid 激活函数，且参数为 Head-Specific
        gate_map = torch.sigmoid(self.gate_linear(x)) 
        
        # 4. 门控融合 (Element-wise multiplication)
        gated_output = attn_output * gate_map
        
        # 5. 最终输出投影
        return self.o_proj(gated_output)
```

#### 2.3.2 性能对比
在 15B 参数的 MoE 模型（15A2B）上进行的实验显示：
* **PPL 提升**：Gated Attention 模型的 PPL 从 Baseline 的 6.026 降低至 5.761。
* **下游任务**：在 MMLU（通用知识）、GSM8k（数学）、HumanEval（代码）等基准测试中均取得了性能提升。

#### 2.3.3 训练稳定性

实验对比了 Baseline 和 Gated Attention 在不同学习率下的训练曲线：
* **Baseline**：在高学习率下容易出现 Loss Spikes，导致训练不稳定。
* **Gated Attention**：显著平滑了 Loss 曲线，几乎消除了 Loss Spikes。这使得模型能够支持更大的 Batch Size 和更高的学习率，从而提升训练效率。

#### 2.3.4 长文本外推能力

在通过 YaRN 技术将上下文窗口从 4k 扩展至 128k 的实验中:

* **Baseline**:随着长度增加,性能出现明显下降,部分原因归结于 Attention Sink 机制在超长上下文下的失效。
* **Gated Attention**:得益于去除了 Attention Sink,模型在长上下文(64k, 128k)下的表现显著优于 Baseline,展现出更强的外推能力。

### 2.4 实施建议

基于论文结论,对于大模型架构优化的建议如下:

1. **推荐配置**:在 Attention 层中,于 SDPA 输出后、输出投影前,插入 **Element-wise Sigmoid Gating**。
2. **超参调整**:由于该结构增强了稳定性,建议在训练时尝试 **适度增大学习率** 以获得更好的收敛效果。
3. **监控指标**:在实验中,可通过观察 Loss 曲线的平滑度以及 Attention Map 中首个 Token 的权重变化来验证改进效果。

---

# 3. Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?

**论文链接**: [https://arxiv.org/abs/2504.13837](https://arxiv.org/abs/2504.13837)

### 摘要

随着 OpenAI o1 和 DeepSeek-R1 的发布，基于验证奖励的强化学习（RLVR, Reinforcement Learning with Verifiable Rewards）被广泛认为是通向"自我进化"和超越基础模型能力（Superhuman Intelligence）的关键路径。行业普遍共识是：类似于 AlphaGo Zero，LLM 可以通过 RL 在庞大的推理空间中探索出全新的策略（如自我反思、回溯等），从而显著提升推理能力边界。一个反直觉且令人警醒的结论：**当前的 RLVR 训练并没有赋予模型全新的推理能力，而仅仅是改变了模型的输出分布，使其更高效地"提取"基础模型（Base Model）中已有的正确推理路径**。

### 3.1 核心发现

#### 3.1.1 效率-容量悖论

通过系统性的 pass@k 评估，发现 RLVR 虽然在小 $k$ 值下（如 $k=1$）表现优异，但在大 $k$ 值下（如 $k=256$），其覆盖的可解决问题范围反而小于基础模型。

#### 3.1.2 分布锐化本质

困惑度（Perplexity）分析表明，RL 模型生成的推理路径完全包含在基础模型的采样分布中，RL 只是提高了这些路径的生成概率，而非创造了新路径。

#### 3.1.3 算法局限性

对比了 PPO、GRPO、Reinforce++ 等多种算法，发现它们都未能触及基础模型的性能上限，且普遍存在推理边界收窄的问题。

### 3.2 评估基准的重构：为何 Greedy Decoding 会骗人？

传统的评估通常使用 Greedy Decoding 或 pass@1 来衡量模型性能，但这只能反映模型的"平均表现"或"最可能的输出"。为了探究模型的潜在推理能力边界（Reasoning Capability Boundary），论文采用了 pass@k 指标。

pass@k 的物理意义是：给予模型 $k$ 次尝试机会，只要有一次做对，就算该问题"可解"。当 $k$ 足够大时，它近似于模型能力的上界。

为了避免高昂的计算成本并降低方差，论文使用了无偏估计器（Unbiased Estimator）来计算 pass@k。

#### 公式呈现

$$\text{pass}@k = \mathbb{E}_{x \sim \mathcal{D}} \left[ 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}} \right]$$

#### 符号拆解

* $n$：总采样次数（在实验中通常设为 128, 256 或 1024）。
* $c$：在 $n$ 次采样中，经过验证器（Verifier）判定为正确的样本数量。
* $k$：我们评估的窗口大小（例如 $k=1, 10, 100$）。
* $\binom{n}{k}$：组合数，表示从 $n$ 个样本中取 $k$ 个的所有可能情况。

#### 物理/数学意义

这行公式计算的是**"从生成的 $n$ 个样本中随机抽取 $k$ 个，其中至少包含一个正确答案"的概率**。$\frac{\binom{n-c}{k}}{\binom{n}{k}}$ 这一项代表了"抽取的 $k$ 个样本全都是错误答案"的概率（因为只有从 $n-c$ 个错误样本中抽取才会全错）。用 $1$ 减去该项，即得到"至少有一个正确"的概率。

该估计器解决了直接进行 $k$ 次采样的方差问题，能够更准确地描绘模型在给定计算预算下的能力边界。

### 3.3 核心悖论：采样效率 vs. 推理边界

实验结果揭示了一个非常有趣的现象，被称为 **"Efficiency-Capacity Paradox"（效率-容量悖论）**。我们观察到两条曲线出现了交叉：

* **小 $k$ 区域（$k < 10$）**：RL 模型显著高于 Base 模型。这说明 RL 成功地将概率密度集中在了正确答案上，极大地提高了采样效率（Sampling Efficiency）。
* **大 $k$ 区域（$k > 100$）**：Base 模型的曲线斜率并未饱和，而是持续上升并最终反超 RL 模型。这说明 Base 模型保留了更多的多样性，能够解决一些 RL 模型因"模式坍缩"（Mode Collapse）而无法解决的困难问题。

这一现象在数学、代码（LiveCodeBench）和视觉推理（MathVista）任务中均一致出现。

#### 结论

RLVR 实际上是在进行一种权衡（Trade-off）——它提高了在常见问题上的命中率（效率），但代价是牺牲了模型探索长尾、低概率正确路径的能力（即缩窄了推理边界）。

### 3.4 困惑度分析：推理路径的"溯源"

为了验证 RL 模型生成的推理步骤（Chain-of-Thought, CoT）是否是全新的，论文进行了困惑度（Perplexity, PPL）分析。

#### 公式呈现

$$PPL_{m}(Y|x) = \exp \left( -\frac{1}{T} \sum_{t=1}^{T} \log P(y_{t} | x, y_{1}, ..., y_{t-1}) \right)$$

#### 符号拆解

* $m$：评估模型（此处使用 Base 模型作为评估者）。
* $Y = (y_1, ..., y_T)$：待评估的 Token 序列（此处为 RL 模型生成的 CoT 路径）。
* $x$：输入的问题 Prompt。
* $P(\cdot)$：模型预测下一个 Token 的概率分布。

#### 物理/数学意义

这个公式衡量了 Base 模型生成 RL 模型产出的那条推理路径的"惊讶程度"。

#### 实验发现

* 随着 RL 训练的进行，$PPL_{Base}(Y_{RL})$ 逐渐降低。
* 这意味着 RL 模型生成的"高分答案"，其实早就潜伏在 Base 模型的低困惑度（高概率）区域中。
* **Deep Insight**：RL 并没有教会模型"无中生有"地学会新的推理逻辑，它只是像一个"放大镜"，把 Base 模型潜意识里由于概率分散而被掩盖的正确路径给"提亮"了。

### 3.5 采样效率鸿沟（Sampling Efficiency Gap）

为了量化当前的 RL 算法距离"完美挖掘 Base 模型潜力"还有多远，论文定义了采样效率鸿沟 $\Delta_{SE}$。

#### 定义

$$\Delta_{SE} = \text{pass}@1(\text{RL Model}) - \text{pass}@K_{max}(\text{Base Model})$$

注：论文中 $K_{max}$ 取 256 作为 Base 模型潜力的近似上界。

#### 实验结论

无论是 PPO、GRPO、Reinforce++ 还是 RLOO，这些算法的 $\Delta_{SE}$ 依然很大且彼此接近（例如在 Omni-MATH 上差距均在 40% 左右）。这意味着：

* 当前的 RL 算法主要是在做"利用"（Exploitation），而非有效的"探索"（Exploration）。
* 将 Base 模型视为 Upper Bound，现有的 RL 方法远未达到最优。

### 3.6 蒸馏（Distillation）与 RL 的本质区别

这是一个关键的对比实验。论文发现，使用强大的教师模型（如 DeepSeek-R1）对小模型进行蒸馏（Distillation/SFT），其 pass@k 曲线是整体上移的，不仅在小 $k$ 处提升，在大 $k$ 处也超越了 Base 模型。

| 方法 | 核心特征 | 推理边界影响 |
|------|----------|--------------|
| RLVR | 在 Base 模型已有的知识分布内进行重排列（Re-weighting） | 容易导致多样性丧失，边界收窄 |
| Distillation | 通过引入外部教师的强推理 Pattern，注入新知识 | 真正扩展模型的推理边界 |

### 3.7 总结与启示

#### 3.7.1 核心结论

这篇论文给当下的 RLVR 热潮降了降温。它指出，目前的 RLVR 范式（如 Zero-RL）本质上是一个"去噪"过程，而非"创造"过程。它通过抑制错误路径、放大正确路径来提升平均分，但这往往以牺牲模型的广度探索能力为代价，导致模型在面对那些 Base 模型原本仅有微弱概率解决的难题时，表现得更加无力（甚至完全不可解）。

#### 3.7.2 局限性与思考

* **Base Model 是天花板**：如果 Base 模型本身无法生成某种推理逻辑（概率极低），现有的 RLVR 很难通过自我探索找到它，因为巨大的动作空间（Action Space）使得盲目探索极其低效。
* **Reward 的稀疏性**：仅靠最终答案的二值奖励（True/False）很难指导模型进行深度的多步推理探索。

#### 3.7.3 未来展望

1. **改进探索机制（Exploration）**：目前的 Token-level 采样探索效率太低，需要引入更高层级的探索策略，如 AlphaEvolve 提出的程序级抽象探索。
2. **数据课程（Data Curriculum）**：通过构建从易到难的问题链，引导模型逐步习得 Meta-skills，从而在更难的任务上获得非零的初始奖励。
3. **细粒度过程奖励（Process Reward）**：从 Outcome Reward 转向 Process Reward (PRM)，为中间推理步骤提供信号，解决信用分配（Credit Assignment）难题。
4. **Agentic RL**：引入多轮交互、工具使用和环境反馈，让模型在真实的"体验"中学习，而非仅仅在静态数据上"刷题"。

---

## 4. Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values

**论文链接**: [https://arxiv.org/pdf/2510.20187](https://arxiv.org/pdf/2510.20187)

### 摘要

在RLHF和DeepSeek-R1等推理模型所采用的RLVR中，一个被长期忽视的问题是：**所有正确答案的价值是相等的吗？**

在传统的 RLVR 范式中，只要模型答对了，奖励通常就是 +1。然而，回答一道小学数学题（2分）和解决一道复杂的微积分题目（10分），其蕴含的"人类价值"显然不同。本文提出的 **RLEV (Reinforcement Learning with Explicit Human Values)** 正是为了解决这一价值对齐问题。

### 4.1 核心价值

* **打破二元奖励**：指出了 RLVR 中"正确即 +1"的局限性，提出将**显式的人类价值信号（如试题分数、难度权重）**直接注入奖励函数。
* **价值敏感的终止策略**：RLEV 训练出的模型获得了一种神奇的 emergent ability（涌现能力）——**看人下菜碟**。对于低价值简单问题，它回答极其简练（节省算力）；对于高价值复杂问题，它会通过更长的思维链（CoT）进行深度推理。
* **鲁棒的 SOTA 表现**：在 7B 和 32B 规模上，RLEV 均优于 REINFORCE++、RLOO 和 GRPO 等纯正确性导向的基线算法，且不仅提高了加权分数，还大幅降低了平均 Token 消耗。

**背景定位**：这是 RLVR 领域的一次"精细化重构"。如果说 RLVR 解决了"模型是否诚实"的问题，RLEV 则进一步解决了"模型是否懂得投入产出比"的问题，是迈向高效、高价值 AI 的重要一步。

### 4.2 效用函数与奖励工程

传统 RLVR 的奖励 $r$ 是二值的：$r \in \{0, 1\}$。RLEV 引入了一个先验假设：**一个回复的效用取决于“正确性”与“问题本身价值”的乘积。**

#### 4.2.1 核心定义

对于 Prompt $x$ 和回复 $y$，人类效用函数 $U(x,y)$ 定义为：

$$U(x,y) = v(x) \cdot \mathbb{I}_{correct}(y)$$

其中，$v(x)$ 是归一化后的人类价值（如考试分数归一化到 [0,1]），$\mathbb{I}_{correct}$ 是正确性指示函数。

#### 4.2.2 工程化奖励函数

直接使用 $v(x)$ 作为奖励会导致训练不稳定，因为低分题目的奖励接近 0，梯度消失，导致模型"懒得学"简单题。作者设计了一个**缩放函数 (Scaling Function)** $s(x)$。

**RLEV 奖励函数**：

$$r(x, y) = s(x) \cdot \mathbb{I}_{correct}(y)$$

$$s(x) = 1 + \min(\alpha \cdot v(x), 1)$$

**符号定义**：

* **$s(x)$ (Scaling Factor)**：这是核心。它保证了只要答对，奖励至少是 1（Base Reward），而对于高价值问题，奖励最高可达 2（Bonus）。
* **$\alpha$ (Hyperparameter)**：调节因子，实验中 $\alpha=10$ 效果最佳。
* **$\min(\cdot, 1)$ (Clipping)**：截断操作，防止极高价值的离群点导致梯度爆炸，破坏训练稳定性。

**数学直觉**：这是一个 **Additive & Clipped**（加性截断）的设计。它没有改变梯度的方向（只奖励正确的），但改变了梯度的**幅度**。这相当于告诉优化器："所有题都要做对，但做对这道压轴题，权重加倍！"

#### 4.2.3 代码实现

```python
def compute_rlev_reward(
    is_correct,  # shape: [B], 0 or 1
    prompt_values, # shape: [B], normalized v(x) in [0, 1]
    alpha #  hyperparameter best is 10
):
    """
    计算 RLEV 奖励。
    """
    # 1. 计算 Scaling Factor s(x)
    # s(x) = 1 + min(alpha * v(x), 1)
    bonus = torch.clamp(alpha * prompt_values, max=1.0)
    scaling_factor = 1.0 + bonus
    
    # 2. 计算最终奖励
    # r = s(x) * I(correct)
    # 如果回答错误，奖励仍为 0；如果正确，奖励被放大
    final_rewards = scaling_factor * is_correct
    
    return final_rewards
```

### 4.3 梯度动力学与 EOS 机制

模型生成 **EOS (End-of-Sequence)** Token 时的梯度。在REINFORCE风格的更新中，对于时间步$t$的Logit $z_k$，梯度为：

**Token 级梯度**：

$$
\frac{\partial J}{\partial z_{k}} = \pi(k|x, y_{<t}) \cdot s(x) \cdot (p_{k} - \bar{p})
$$

**符号定义**：

* **$z_e$ (EOS Logit)**：特指 EOS token 的 logit。
* **$p_e$**：在当前位置结束并被判定为"正确"的概率。
* **$\bar{p}_{\neg e}$**：如果不结束（继续生成），未来能答对的平均期望概率。
* **$s(x)$**：人类价值的缩放因子。

**推导出的 EOS 梯度**：

$$
\frac{\partial J}{\partial z_{e}} = s(x) \cdot \pi_{e}(1-\pi_{e}) \cdot (p_{e} - \bar{p}_{\neg e})
$$

#### 4.3.1 直觉与物理意义

这个公式揭示了一个**竞争机制**：模型在每一步都在权衡"现在闭嘴（EOS）能对吗？" vs "再多说两句（Continue）能对吗？"。

1. **当 $p_e > \bar{p}_{\neg e}$ 时**（现在结束赢面更大）：梯度为正，推高 EOS 概率。
2. **当 $p_e < \bar{p}_{\neg e}$ 时**（还没说清楚，继续说赢面大）：梯度为负，抑制 EOS。

**RLEV 的 $s(x)$ 做了什么？**

它充当了一个**放大器 (Amplifier)**：

* 对于**简单题**（低 $v(x)$，但通常 $p_e$ 很容易就大于 $\bar{p}_{\neg e}$），$s(x)$ 适度放大正梯度，鼓励模型尽快结束，避免啰嗦。
* 对于**难题**（高 $v(x)$），模型往往需要推理。在推理未完成时，$p_e$ 远小于 $\bar{p}_{\neg e}$（现在结束必错）。此时，$s(x)$ 变得很大（接近 2），**极大地放大了负梯度**，强力抑制 EOS。这迫使模型继续生成，直到它有足够的信心认为答案完整了。

这就是为什么 RLEV 模型展现出了**价值敏感的终止策略**：高分题不敢轻易交卷，低分题绝不浪费笔墨。

### 4.4 实验结果与洞察

#### 4.4.1 实验表现

作者在包含 100k 考试题目的数据集上训练，并在 English/Chinese benchmarks (如 GPQA, C-Eval) 上测试。

| Metric | Baseline (Correctness Only) | **RLEV (Ours)** | **Delta** |
|:---|:---|:---|:---|
| **Acc (7B)** | 63.8% | **65.3%** | +1.5% |
| **H-Acc (加权准确率)** | 55.0% | **57.0%** | +2.0% |
| **Resp. Length (32B)** | 226.2 tokens | **68.7 tokens** | **-69.6% (Efficiency!)** |
| **Value Density** | 0.25 | **0.90** | **+260%** |

**关键结论**：

1. **精度提升**：即使是普通准确率也有提升，说明区分题目重要性有助于模型分配参数容量。
2. **极致效率**：Response Length 大幅缩短。RLEV 极大程度上抑制了模型在简单问题上的"废话文学"。

#### 4.4.2 消融实验

对比了两种替代方案，证明了效果确实来自"价值对齐"，而非单纯的"奖励变大"。

* **Uniform Scaling (全部奖励 x 1.2)**：效果**下降**。说明单纯放大奖励（相当于调大学习率）会破坏训练稳定性。
* **Random Weights (随机分配价值)**：准确率无显著提升，且回复长度没有变短。说明模型确实学到了$v(x)$ 与题目内容的关联，而不是拟合了一个全局的长度偏置。

### 4.5 批判性思考与落地建议

#### 4.5.1 数据依赖性

RLEV 的前提是我们要有$v(x)$。在考试场景下这很容易（题目自带分数）。但在通用的Chat场景或Code场景下，谁来定义"这个Prompt价值10分，那个价值2分"？

**解决方案**：训练了一个由 LLM 驱动的 **Score Predictor** 来打分。实验显示，即使使用预测的（有噪声的）分数，RLEV 依然优于 Baseline。这证明了该方法的鲁棒性。

#### 4.5.2 静态价值 vs 动态意图

目前的 $v(x)$ 是静态绑定的。但在真实对话中，用户的急迫程度是动态的。

**思考**：未来的改进方向可能是将 $v(x)$ 作为一个Condition输入给Reward Model，或者根据User Prompt中的tone（语气）动态生成$v(x)$。

#### 4.5.3 对推理的影响

RLEV 训练出的模型倾向于更短的回复（在大多数情况下）。这对于降低推理成本（Time to First Token, Total Generation Time）是巨大的利好。这实际上是一种**隐式的模型蒸馏**，让模型学会用最少的 Token 传达最大的价值。

### 4.6 总结

RLEV用极小的算法改动（修改Reward计算逻辑），换取了模型行为层面的重大优化（价值感知 + 高效输出）。

---

## 5. DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models


**论文链接**: [https://arxiv.org/abs/2512.02556](https://arxiv.org/abs/2512.02556)

---

## 6. Training LLMs for Honesty via Confessions

**论文链接**: [https://cdn.openai.com/pdf/6216f8bc-187b-4bbb-8932-ba7c40c5553d/confessions_paper.pdf](https://cdn.openai.com/pdf/6216f8bc-187b-4bbb-8932-ba7c40c5553d/confessions_paper.pdf)

---

## 7. The Art of Scaling Reinforcement Learning Compute for LLMs

**论文链接**: [https://arxiv.org/abs/2510.13786](https://arxiv.org/abs/2510.13786)

---


## 8. Twilight: Adaptive Attention Sparsity with Hierarchical Top-p Pruning

**论文链接**: [https://arxiv.org/abs/2502.02770](https://arxiv.org/abs/2502.02770)

---