---
layout: post
title: "GPT和Qwen系列"
date: 2025-10-02
tags: [LLM]
comments: true
author: 炼丹怪
---

本文解析了GPT与Qwen系列的演进历程。GPT系列确立了预训练范式与Scaling Law，引领了从文本生成到多模态推理的突破；Qwen系列作为开源先锋，凭借MoE架构、海量数据及“思维预算”机制，在效率与逻辑推理上实现飞跃。两者共同推动了LLM向通用人工智能的加速迈进。

---

<!-- more -->

## 1. GPT系列：生成式预训练的开始

GPT（Generative Pre-trained Transformer）系列的发展史，本质上是深度学习领域对"通用性"不断追求的历史。该系列通过持续验证一个核心假设：如果在足够多样化的文本语料库上训练一个高容量的语言模型，该模型将能够以无监督的方式学习到语言的深层结构和世界知识。

### 1.1 GPT-1：通过生成式预训练提升语言理解能力

GPT-1的诞生正值NLP领域的瓶颈期。当时，大多数深度学习方法严重依赖大量的手动标注数据，这限制了模型在缺乏标注资源的领域中的应用。GPT-1的核心创新在于提出了一种半监督学习方法，将无监督的生成式预训练与有监督的判别式微调相结合，从而打破了这一数据瓶颈。

#### 1.1.1 架构选择与训练目标

GPT-1选择了Transformer的解码器（Decoder）作为核心架构，而非当时流行的循环神经网络（RNN）或长短期记忆网络（LSTM）。Transformer架构提供了更结构化的记忆机制，能够更好地处理文本中的长距离依赖关系，从而实现更稳健的迁移性能。

训练过程分为两个阶段：
1. 首先在大量无标注文本上优化语言建模目标，学习初始参数
2. 随后使用有监督目标将这些参数适应到具体的目标任务中

具体的训练目标是最大化以下似然函数：

$$L_1(\mathcal{U}) = \sum_i \log P(u_i | u_{i-k}, \dots, u_{i-1}; \Theta)$$

其中 $k$ 是上下文窗口的大小。GPT-1使用了12层的Transformer解码器，包含768维的隐藏状态和12个注意力头。训练语料采用了BooksCorpus数据集，该数据集包含超过7000本未出版的书籍，涵盖冒险、幻想和浪漫等多种体裁。这一数据集的关键特性在于其包含长段的连续文本，这对于生成模型学习长距离信息至关重要。

#### 1.1.2 任务感知输入变换

GPT-1的另一个重要贡献是引入了**任务感知的输入变换（Task-aware input transformations）**。传统的迁移学习往往需要针对每个目标任务大幅修改模型架构。相比之下，GPT-1通过遍历式（traversal-style）方法，将文本蕴含、问答、语义相似度等结构化输入转换为预训练模型可以处理的有序序列。

例如，对于文本蕴含任务，前提（premise）和假设（hypothesis）被连接成一个序列，中间用分隔符隔开。这种方法使得模型能够在微调过程中只需对架构进行极小的更改**通常仅增加一个线性输出层**即可实现有效的迁移。

#### 1.1.3 实验结果与启示

实验结果表明，GPT-1在所研究的12项任务中有9项达到了最先进水平（SOTA）。例如，在常识推理任务（Stories Cloze Test）上，准确率绝对提升了8.9%；在问答任务（RACE）上提升了5.7%。

这证明了生成式预训练不仅能学习到词级统计信息，还能捕获**更高层次的语义和句法结构**，这些知识可以有效地迁移到各种下游任务中，即使这些任务与预训练语料的领域并不完全一致。

### 1.2 GPT-2：语言模型即无监督多任务学习者

如果说GPT-1证明了预训练对监督微调的增益，那么GPT-2则提出了一个更为激进的假设：语言模型本质上是无监督的多任务学习者。GPT-2试图证明，在没有任何显式监督的情况下，仅仅通过在大规模、多样化的数据集上训练，语言模型就能学会执行翻译、摘要、问答等任务。

#### 1.2.1 零样本任务迁移的理论基础

GPT-2的核心理念是，学习执行单个任务可以从概率上表示为估计条件分布

$$P(\text{output} | \text{input})$$

而通用系统应该能够对<输入，任务>进行建模。

$$P(\text{output} | \text{input}, \text{task})$$


由于语言本身提供了一种灵活的方式将任务、输入和输出指定为符号序列（例如，"将英语翻译成法语：[英语句子] => [法语句子]"），语言建模实际上是在隐式地进行多任务学习。当模型容量足够大且数据足够丰富时，模型应该能够通过**零样本（Zero-shot）**的方式直接执行下游任务。

#### 1.2.2 WebText数据集的构建与质量控制

为了验证这一假设，OpenAI构建了一个新的数据集WebText。以往的研究主要使用新闻文章、维基百科或小说，或者使用质量参差不齐的Common Crawl。GPT-2的研究者意识到数据质量的重要性，因此他们采取了一种新颖的筛选策略：抓取Reddit上获得至少3个Karma（点赞）的出站链接。这可以被视为一种启发式的人类过滤，表明其他用户认为该链接内容是有趣或有价值的。

最终得到的WebText数据集包含超过800万个文档，总计40GB的文本。值得注意的是，为了避免评估时的数据泄露，维基百科文章被特意从训练集中排除。

#### 1.2.3 规模化带来的能力涌现

GPT-2在架构上与GPT-1类似，但进行了细节调整，例如将层归一化（Layer Normalization）移至每个子块的输入处，并在最终的自注意力块后添加了额外的层归一化。模型参数量最大增加到了15亿（1.5B）。

实验结果显示，GPT-2在8个语言建模数据集中的7个上达到了SOTA，且在零样本设置下表现出色。特别是在Winograd Schema Challenge（旨在衡量常识推理能力）中，GPT-2将准确率提升了7%，达到70.7%。

然而，GPT-2在某些任务（如摘要生成）上的表现虽然定性上展示了能力，但定量指标仍仅略高于随机基线。这揭示了零样本迁移的局限性，但也指出了方向：随着模型容量的增加，性能呈对数线性增长。这暗示了通过进一步扩大规模，模型能力尚未达到天花板。

### 1.3 GPT-3：语言模型是少样本学习者

GPT-3的发布标志着大模型时代的正式到来。该模型拥有1750亿参数，比之前的非稀疏语言模型大10倍以上。GPT-3的研究重心不再是架构创新，而是验证模型规模、数据规模与计算量之间的比例定律（Scaling Laws），并引入了"上下文学习"（In-Context Learning）这一全新范式。

#### 1.3.1 上下文学习（In-Context Learning）

GPT-3摒弃了GPT-1中的微调步骤，转而采用上下文学习。在这种模式下，模型在推理时被赋予一个包含任务描述和数个示例（Demonstrations）的自然语言提示（Prompt），模型需要根据这些上下文信息补全最后一个示例。这一过程不涉及任何梯度更新。

GPT-3在三种设置下进行了评估：
- **零样本（Zero-shot）**：仅提供任务描述
- **单样本（One-shot）**：提供任务描述和一个示例
- **少样本（Few-shot）**：提供任务描述和尽可能多的示例（通常10-100个），直到填满上下文窗口

#### 1.3.2 性能表现与规模效应

评估结果令人震惊。在TriviaQA基准测试中，GPT-3在少样本设置下的准确率达到71.2%，超越了当时在相同闭卷设置下经过微调的SOTA模型。在LAMBADA数据集（测试长距离依赖建模）上，GPT-3达到了76%的准确率，比之前的SOTA提升了8%。

更重要的是，数据表明模型性能与参数规模之间存在平滑的幂律关系。小模型从上下文示例中获益甚微，而1750亿参数的GPT-3则展现出强大的元学习（Meta-learning）能力，能够迅速适应上下文中定义的新任务。

#### 1.3.3 数据污染与合成文本风险

由于使用了Common Crawl等网络数据集，GPT-3面临严重的数据污染问题（即测试集出现在训练集中）。研究团队开发了复杂的过滤工具来量化这一影响，虽然发现大部分任务受污染影响较小，但也标记了部分可能因记忆而导致分数虚高的数据集。

此外，GPT-3生成的合成新闻文章质量极高，人类评估者辨别真伪的准确率仅为52%（接近随机猜测），这引发了关于大规模虚假信息生成的社会伦理担忧。

### 1.4 GPT-4：多模态与专业能力的飞跃

GPT-4是GPT系列的集大成者，它不仅是一个大规模的语言模型，更是一个多模态模型，能够接受图像和文本输入并产生文本输出。GPT-4的开发重点在于实现可预测的扩展（Predictable Scaling）和通过后训练对齐（Post-training Alignment）提升安全性与有用性。

#### 1.4.1 人类水平的专业考试表现

GPT-4最引人注目的成就是其在人类专业考试中的表现。在模拟律师资格考试（Uniform Bar Exam）中，GPT-4的得分进入了前10%，而GPT-3.5仅处于倒数10%。在LSAT、SAT数学以及多项AP考试中，GPT-4均取得了高分位数的成绩。

这一结果证明了通用预训练模型已经具备了处理复杂逻辑、法律推理及跨学科知识的能力。

#### 1.4.2 视觉输入与多模态理解

GPT-4引入了视觉输入能力，这使得模型能够理解图像内容（如解释梗图、读取图表数据）。例如，当输入一张包含炸鸡块拼成的世界地图的图片并询问含义时，GPT-4能够准确识别出这是用炸鸡块模拟的地图，并解释其中的幽默感。

这种跨模态理解能力极大地扩展了模型的应用场景。

#### 1.4.3 可预测的扩展与安全对齐

GPT-4项目的核心工程挑战之一是构建能够在广泛尺度上表现可预测的深度学习基础设施。研究团队能够利用仅消耗1/1000计算量的较小模型来准确预测GPT-4的最终损失值和部分能力（如HumanEval代码生成通过率）。这对于在大规模训练前进行资源规划和风险评估至关重要。

在安全性方面，GPT-4引入了基于规则的奖励模型（RBRMs）进行强化学习微调（RLHF）。这套系统通过零样本分类器来指导模型拒绝有害请求（如制造炸弹的教程），同时避免拒绝无害请求。经过这一流程，GPT-4响应违规内容的倾向比GPT-3.5降低了82%。

## 2. Qwen系列：开源生态的效率革命与推理进阶

如果说GPT系列定义了闭源模型的上限，那么Qwen系列则代表了开源模型在效率、多语言能力及推理深度上的快速迭代与突破。从Qwen1的基础建设，到Qwen2的架构优化，再到Qwen3引入的思维预算机制，该系列展示了一条清晰的技术进化路线。

### 2.1 Qwen(v1)：构建全面的开源基座

Qwen系列的首次发布旨在提供一组全面的语言模型，涵盖从18亿到140亿参数的不同规模，以满足不同开发者的需求。该系列包括基础预训练模型（QWEN）和经过人类对齐微调的聊天模型（QWEN-CHAT）。

#### 2.1.1 基础架构与训练数据

Qwen采用了改进的Transformer架构。在位置编码方面，选择了RoPE（旋转位置编码）以处理变长序列，并使用FP32精度计算逆频率矩阵以确保高精度。激活函数选用了SwiGLU，归一化层采用了RMSNorm，并使用了预归一化（Pre-Norm）策略以提高训练稳定性。

训练数据方面，Qwen构建了一个包含3万亿token的数据集，涵盖多语言文本（重点是中文和英文）、代码和数学内容。数据预处理流程包括基于MinHash和LSH的模糊去重，以及基于规则和机器学习模型的质量过滤。

#### 2.1.2 词表优化与压缩效率

Qwen使用了一种基于字节对编码（BPE）的分词器，词表大小约为152K。与XLM-R、LLaMA等其他分词器相比，Qwen的分词器在多种语言（包括中文、英文及泰语、希伯来语等）上实现了更高的压缩率。这意味着在相同的序列长度限制下，Qwen能够容纳更多的信息，从而显著降低了推理成本。

#### 2.1.3 工具使用与代码解释器

Qwen-Chat模型的一个显著特点是其强大的工具使用（Tool Use）和代理（Agent）能力。通过ReAct提示技术，Qwen能够有效地使用未见过的工具。

特别是在代码解释器（Code Interpreter）任务中，Qwen展示了通过编写和执行Python代码来解决数学问题、进行数据可视化和通用任务处理的能力。在内部基准测试中，Qwen-14B-Chat在代码可执行率和结果正确性上均大幅领先于同等规模的开源模型（如Code Llama-Instruct）。

### 2.2 Qwen2：架构精进与自动化对齐

Qwen2系列标志着模型架构的成熟和后训练流程的自动化。该系列将参数范围扩展至0.5B到72B，并引入了混合专家（MoE）模型。

#### 2.2.1 混合专家模型（MoE）与注意力机制优化

Qwen2引入了Qwen2-57B-A14B模型，这是一种MoE架构，总参数量为570亿，但在推理时每次仅激活140亿参数。为了提高效率，Qwen2采用了细粒度专家（Fine-grained Experts）策略，即创建更小规模的专家并同时激活更多专家，从而提供更丰富的专家组合。

在注意力机制上，Qwen2全系列采用了分组查询注意力（GQA），显著降低了推理时的KV缓存占用，提高了吞吐量。针对长上下文（最长可达131,072 tokens），Qwen2结合了YARN机制和双块注意力（Dual Chunk Attention, DCA），有效地实现了长度外推。

#### 2.2.2 自动化数据合成与可扩展对齐

在后训练阶段，Qwen2大幅减少了对人工标注的依赖，转向自动化数据合成。这包括：

1. **拒绝采样（Rejection Sampling）**：针对数学任务，利用LLM生成多条推理路径，通过验证最终答案的正确性来筛选高质量数据。
2. **执行反馈（Execution Feedback）**：针对代码任务，利用LLM生成代码及测试用例，通过代码编译和执行的结果来生成偏好数据。
3. **指令进化（Instruction Evolution）**：利用模型自身对现有指令添加约束或要求，增加指令的复杂度和多样性。

这种高度自动化的流程使得Qwen2能够在极少人工干预的情况下，通过超过50万条高质量示例进行微调，从而在MMLU、GSM8K等基准上取得优异成绩。

### 2.3 Qwen2.5：数据规模的胜利与强化学习的深化

Qwen2.5是Qwen系列的一次重大升级，其核心驱动力在于预训练数据规模的巨大飞跃和强化学习流程的精细化。

#### 2.3.1 18万亿Token的预训练语料

Qwen2.5将预训练数据量从Qwen2的7万亿提升到了18万亿token。这一规模的扩张为模型提供了更坚实的常识、专家知识和推理能力基础。

在数据配比上，Qwen2.5通过Qwen2-Instruct模型对内容进行分类和平衡，对科学、技术等高价值领域进行上采样，同时对电商、社交媒体等重复性内容进行下采样。此外，Qwen2.5还大量引入了由Qwen2-Math和Qwen2-Coder生成的合成数据，形成了一个自我强化的数据反馈循环。

#### 2.3.2 两阶段强化学习流程

Qwen2.5实施了更为复杂的后训练策略，包含两个阶段的强化学习：

1. **离线RL（Offline RL）**：重点针对具有明确标准答案的任务（如数学、代码）。利用DPO（直接偏好优化）处理那些奖励模型难以评估但可以通过执行验证的任务。
2. **在线RL（Online RL）**：利用GRPO（分组相对策略优化）来优化模型在主观任务上的表现，如有用性、简洁性和安全性。在线RL通过奖励模型对模型生成的多个响应进行评分，并优先训练那些方差较大的查询，以提高学习效率。

评估结果显示，旗舰模型Qwen2.5-72B-Instruct在多项基准测试中表现优异，甚至能够与其参数量大5倍的Llama-3-405B-Instruct相抗衡，充分证明了高质量数据和精细化后训练的有效性。

### 2.4 Qwen3：思维模式的统一与计算预算的动态分配

Qwen3代表了Qwen系列的最新前沿，它引入了"思考模式"（Thinking Mode）和"非思维模式"（Non-Thinking Mode）的统一框架，试图解决快思考（System 1）与慢思考（System 2）在AI系统中的融合问题。

#### 2.4.1 统一的思维框架与思维预算

Qwen3的一项关键创新是将处理复杂多步推理的"思维模式"和处理快速上下文响应的"非思维模式"集成在同一个模型中。用户无需在专门的推理模型（如QwQ）和聊天模型之间切换，而是可以根据查询的复杂度动态调整。

更具革命性的是"思维预算"（Thinking Budget）机制的引入，允许用户在推理过程中自适应地分配计算资源。对于复杂的数学证明，可以增加预算以进行深度推理；对于简单的闲聊，则可以减少预算以降低延迟。

#### 2.4.2 36万亿Token与多模态数据提取

Qwen3进一步将预训练数据规模翻倍至36万亿token，涵盖119种语言。为了获取高质量的学术和技术内容，Qwen3采用了多模态方法，利用Qwen2.5-VL模型从海量PDF文档中提取文本，确保了训练数据的深度和专业性。

预训练过程分为三个阶段：
1. 通用知识阶段（30T token）
2. 推理增强阶段（5T token，包含大量STEM和合成数据）
3. 长上下文阶段（将上下文长度从4k扩展至32k）

#### 2.4.3 强对弱蒸馏（Strong-to-Weak Distillation）

为了在较小规模模型上实现高性能，Qwen3采用了"强对弱蒸馏"策略。利用旗舰模型（如Qwen3-235B-A22B MoE）生成的知识来训练较小的密集模型（如Qwen3-14B, Qwen3-32B）。

这种方法使得Qwen3-32B在推理任务上超越了之前的推理专家模型QwQ-32B。Qwen3的MoE架构也极其高效，旗舰模型虽有2350亿参数，但每次推理仅激活220亿参数，兼顾了巨大的模型容量和推理速度。

### 3.1 数据规模与质量的双重飞跃

从GPT-1的BooksCorpus（约10亿词）到GPT-3的3000亿token，再到Qwen2.5的18万亿和Qwen3的36万亿token，数据规模呈现指数级增长。这一趋势证实了LLM的"数据饱和点"尚未到来，只要数据足够多样化且质量够高。

然而，单纯的数据堆砌已不再足够。从GPT-2开始对WebText进行人工筛选，到Qwen3利用视觉模型提取PDF数据并大量使用合成数据，数据工程已从简单的清洗转向了复杂的内容生产和精细化配比。合成数据（Synthetic Data）正成为突破自然数据稀缺瓶颈的关键。

### 3.2 架构的收敛与效率分化

在架构层面，Transformer解码器已成为绝对的主流。然而，为了应对规模和效率的挑战，架构细节发生了显著分化：

- **位置编码**：从GPT早期的学习位置嵌入转向了Qwen系列的RoPE，以更好地处理长序列及实现长度外推。
- **归一化与激活**：为了训练稳定性，Pre-Norm和RMSNorm逐渐取代了传统的LayerNorm；SwiGLU因其优越的性能表现取代了GeLU。
- **计算效率**：混合专家（MoE）架构（如Qwen2/3中的实现）成为平衡模型总容量（知识存储）与推理成本（计算延迟）的关键技术。它允许模型在不显著增加推理算力的情况下，大幅扩展参数规模。

### 3.3 推理能力的显性化与可控化

推理能力的进化是近年来最显著的突破。GPT-3通过少样本提示激发推理，GPT-4通过RLHF强化推理，而Qwen3则通过引入"思维模式"将推理过程显性化。

这种将"生成推理路径"与"生成最终答案"解耦，并引入"思维预算"的做法，标志着大模型正试图模拟人类的"系统2"思维（慢思考）。这不仅提高了复杂任务的准确率，也为用户提供了对模型计算成本的细粒度控制。

### 3.4 模型参数与性能对比表

下表总结了本报告中讨论的关键模型的参数规模、训练数据量及核心创新点，以直观展示技术演进的脉络。

| 模型系列 | 核心创新点 | 预训练规模 | 架构亮点 | 主要对齐方法 |
|---------|-----------|-----------|---------|-------------|
| GPT-1 | 预训练+微调范式 | BooksCorpus | Transformer Decoder | 有监督微调 (SFT) |
| GPT-2 | 零样本迁移 | WebText (40GB) | Pre-Activation LayerNorm | 无 (无监督) |
| GPT-3 | 上下文学习 (Few-shot) | 300B Tokens | Sparse Attention (Local) | 无 (Base model) |
| GPT-4 | 多模态, 可预测扩展 | 未公开 | 未公开 (Multimodal) | RLHF + RBRMs |
| Qwen (v1) | 工具使用/代码解释器 | 3T Tokens | RoPE, SwiGLU | SFT + RLHF |
| Qwen2 | 自动化可扩展对齐 | 7T Tokens | GQA, MoE, DCA | 协作数据标注 |
| Qwen2.5 | 大规模数据扩展 | 18T Tokens | MoE + Dense | 离线 (DPO) + 在线 (GRPO) |
| Qwen3 | 思维预算/双模式 | 36T Tokens | MoE (235B/22B active) | 强对弱蒸馏 (Strong-to-Weak) |

## 4. 结论

从GPT-1到Qwen3的演进历程，不仅是模型参数量的数字游戏，更是人工智能从统计语言建模向通用推理智能体迈进的宏大叙事。

GPT系列确立了缩放定律和上下文学习的基础范式，证明了规模本身即是能力涌现的源泉。而Qwen系列则成功地将这些能力民主化，通过高效的MoE架构、精细的数据工程和创新的推理机制，证明了开源模型完全具备与顶级闭源系统分庭抗礼的实力。

Qwen3中"思维模式"与"思维预算"的引入，暗示了LLM发展的下一个阶段：模型将不再是静态的文本生成器，而是动态的计算系统。它们将能够根据任务的难度，自主调节计算资源的投入，实现从直觉反应到深度逻辑推理的平滑过渡。

这种动态性，结合多模态感知和工具使用能力，正将我们带向一个更接近通用人工智能（AGI）的未来——一个既强大又高效，且具备高度可控性的智能未来。